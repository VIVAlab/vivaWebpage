<!DOCTYPE HTML>
<!--
	Halcyonic by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Long Hands Gesture Recognition: Real-Time Hand Gesture Recognition and Hand Tracking</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="/research/viva/assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="/research/viva/assets/css/main.css" />
		<link rel="stylesheet" href="/research/viva/assets/css/bootstrap.min.css" />
		<!--[if lte IE 9]><link rel="stylesheet" href="/research/viva/assets/css/ie9.css" /><![endif]-->

			<script src="/research/viva/assets/js/jquery.min.js"/>
			<script src="/research/viva/assets/js/main.js"></script>
			<script src="/research/viva/assets/js/skel.min.js"></script>
			<script src="/research/viva/assets/js/skel-viewport.min.js"></script>
			
			<script src="/research/viva/assets/js/util.js"></script>
			
			<!--[if lte IE 8]><script src="/research/viva/assets/js/ie/respond.min.js"></script><![endif]-->
			
			<script src="/research/viva/assets/js/bootstrap.min.js"></script>

			<script>
			$(function(){
			  $(".template-header-container").load("/research/viva/header-template.html"); 
			  $(".template-footer-container").load("/research/viva/footer-template.html"); 
			});

			function collapsejump(evt)
			{
				elem = $("#collapse-trigger");
				if (elem.attr("aria-expanded") === undefined || elem.attr("aria-expanded") == "false")
				{
					targ = $(evt.target.hash)
					$("#collapse1").on('shown.bs.collapse', function () {
						targ[0].scrollIntoView();
					});
					$("#collapse1").collapse("toggle");
				}
			}

			function collapsetrig()
			{
				elem = $("#collapse-trigger");
				if (elem.attr("aria-expanded") === undefined || elem.attr("aria-expanded") == "false")
				{
					elem.html('Click here for less');
				}
				else
				{
					elem.html('Click here for more');
				}
			}
			</script>

		<style type="text/css">
			.tg  {border-collapse:collapse;border-spacing:0;margin:0px auto;}
			.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;}
			.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;}
			.tg .tg-s6z2{text-align:center}
			@media screen and (max-width: 767px) {.tg {width: auto !important;}.tg col {width: auto !important;}.tg-wrap {overflow-x: auto;-webkit-overflow-scrolling: touch;margin: auto 0px;}}

			.hmscreencap { position: relative;border: 1px solid #333;margin: 2%;overflow: hidden;width: 640px; }
			.hmscreencap img { max-width: 100%; -moz-transition: all 0.5s; -webkit-transition: all 0.5s; transition: all 0.5s; }
			.hmscreencap:hover img { -moz-transform: scale(2.0); -webkit-transform: scale(2.0); transform: scale(2.0); }

			.hmscreencap_additional { position: relative;border: 1px solid #333;margin: 2%;overflow: hidden;width: 640px; }
			.hmscreencap_additional img { max-width: 100%; -moz-transition: all 0.5s; -webkit-transition: all 0.5s; transition: all 0.5s; }
			.hmscreencap_additional:hover img { -moz-transform: scale(1.2); -webkit-transform: scale(1.2); transform: scale(1.2); }

			.center {text-align:left;width:640px;margin:0 auto;}
			.center ul {font-family:Arial, sans-serif;font-size:14px;list-style-type: disc;}
		</style>
	</head>
	<body >
			<!-- Header -->
			<div class="template-header-container"></div>

		<div id="page-wrapper">

			<!-- Content -->
			<div id="content-wrapper"><div id="content"><div class="container"><div class="row"><div class="9u 12u(mobile)">

			<!-- Main Content -->
			<section>
				<header>
					<h2><center>Long Hands Gesture Recognition: Real-Time Hand Gesture Recognition and Hand Tracking</center></h2>

					<div class="tg-wrap"><table class="tg">
  <tr>
    <th class="tg-s6z2"><a href="mailto:ppopo068@uottawa.ca?Subject=Hand Gesture Recognition">Pavel Popov</a>, <a href="mailto:laganier@eecs.uottawa.ca?Subject=Hand Gesture Recognition">Robert Laganiere</a><br>University of Ottawa<br>Ottawa, ON, Canada</th>
      </tr>
  <tr>
    <td class="tg-s6z2" colspan="2">Questions? <a href="mailto:rverb054@uottawa.ca?Subject=HandGestureRecognition&cc=laganier@eecs.uottawa.ca;ppopo068@uottawa.ca">Drop us a line</a></td>
  </tr>
</table></div>
				</header>

				<hr>
				<center><a id="dataset_handtrack" name="dataset"><h3>Long Hands Gesture Recognition and Hand Tracking: Hand Tracking Dataset</h3></a></center>
				<hr>

A dataset used to test the hand tracking capabilities of the Long Hands Gesture Recognition and Hand Tracking system is available <a href="https://www.dropbox.com/s/rgtvul462escdjh/GR%20Hand%20Tracking%20Dataset.rar?dl=1">here</a>. The dataset contains videos of 5 different users, as well as hand location annotations, and finger count and specification information. A demonstration of the Long Hands Gesture Recognition and Hand Tracking system which is capable of automatic user registration and hand tracking in a 2D video only context is available <a href="https://www.youtube.com/watch?v=6ICK1ELZmfE">here</a>.

				<hr>
				<center><a id="dataset_gesture" name="dataset"><h3>Long Hands Gesture Recognition: Gesture Recognition Dataset</h3></a></center>
				<hr>

A dataset of 5 gestures and negative background samples used for the machine learning on this project can be found <a href="https://www.dropbox.com/s/9tvvrg34w6vr0ht/GR_Dataset.zip?dl=1">here</a>. The negative background samples were enriched with additional images from publically available datasets [1],[2].<br/><br/>

<b>References</b><br/>
1. Faces 1999. <a href="http://www.vision.caltech.edu/archive.html">http://www.vision.caltech.edu/archive.html</a><br/>
2. Quattoni, A., & Torralba, A. Indoor Scene Recognition. <a href="http://web.mit.edu/torralba/www/indoor.html">http://web.mit.edu/torralba/www/indoor.html</a>

				<hr>
				<center><a id="overview" name="overview"><h3>Overview</h3></a></center>
				<hr>

Real-time Hand Gesture Recognition is interesting and power way  for users to interface with computers. It is a multi-faceted problem that has many different competing approaches. Our approaches focus on:<br><br>

<div class="center"><ul>
<li>Automatic Colour Image Processing and Segmentation Techniques</li>
<li>Real-time Performance</li>
<li>Robust hand gesture recognition using machine learning and hand contour shape analysis</li>
<li>Robust hand tracking using hand contour shape analysis and template matching</li>
</ul></div>

This project aims to create powerful hand gesture recognition and hand tracking approaches and to combine them into robust real-time user hand gesture interfaces.


<br><br>

		      
		    <a id="collapse-trigger" data-toggle="collapse" href="#collapse1" onclick="collapsetrig()">Click here for more</a>
		      
		    <div id="collapse1" class="panel-collapse collapse">
<hr>
<center><a id="examples" name="examples"><h3>Projects that have contributed to this research direction</h3></a></center>
<hr><br>
Several projects have contributed to this research direction.<br>
<div class="center"><ul>
<li>Hand Gesture Recognition for car driver interactions: This project achieved position independent hand gesture recognition using machine learning.</li>
<li>Long Hands Controller for Halo: This controller proved the viability of my gesture recognition system for video game control applications</li>
<li>Long Hands Gesture Recognition System: This project achieved robust fixed position hand gesture recognition and hand tracking</li>
<li>Gesture Recognition Paint: This project successfully ported an earlier version of my gesture recognition work on an Android tablet. It allows a user to draw pictures using Gesture Recognition</li>
</ul></div>

<br>



<hr>
<center><a id="examples2" name="examples2"><h3>Hand gesture recognition for car driver interactions.</h3></a></center>





<center><a name="HGRCarDriverSubHeader"><h4>MITACS Accelerate Project 2017</h4></a></center>

<div class="tg-wrap"><table class="tg">

    <th class="tg-s6z2"><a href="mailto:ppopo068@uottawa.ca?Subject=Hand Gesture Recognition">Pavel Popov</a>, <a href="mailto:laganier@eecs.uottawa.ca?Subject=Hand Gesture Recognition">Robert Laganiere</a><br>University of Ottawa<br>Ottawa, ON, Canada</th>
    <th class="tg-s6z2"><a href="http://klashwerks.com/">Klashwerks Inc</a>, <br>Collaborating Partner<br>Ottawa, ON, Canada</th>
      </tr>
  <tr>
    <td class="tg-s6z2" colspan="2">Questions? <a href="mailto:rverb054@uottawa.ca?Subject=HandGestureRecognition&cc=laganier@eecs.uottawa.ca;ppopo068@uottawa.ca">Drop us a line</a></td>
  </tr>
</table></div>

<hr><br>

This project was the result of a MITACS Accelerate research internship with a partnering company called Klashwerks. The company wanted to have a gesture recognition interface for their mobile device built for the connect car market. During this internship I improved my existing Hand Tracking algorithm by adding hand pose recognition capabilities using HOG Cascade machine learning. Using what I learned with HOG Cascades I then created a robust real-time Hand Gesture Recognition system using HOG Cascades capable of recognizing specific static hand gestures anywhere in a video frame. The system performed well on PC. The Hand Gesture Recognition system was trained with a small sample dataset. I also added an option to the Hand Gesture Recognition system to use HOG SVMs instead of HOG Cascades in order to make speed and performance comparisons with a competing machine learning method. The Hand Gesture Recognition system was attempted to be ported to Klashwerks's mobile device for the connected car market. However the Hand Gesture Recognition system achieved low performance on the mobile device and was replaced with a simpler proximity sensor. This research internship also produced a very large hand gesture image dataset that will be crucial in future research that I do with machine learning methods for my PhD. MITACS Accelerate is a research funding program run by MITACS.
<br><br>
<left><a name="Results"><h4>Results</h4></a></left>


Robust real-time Hand Gesture Recognition system capable of recognizing 3 different hand gestures anywhere in a video frame.
<br><br>

<center><iframe width="640" height="360" src="https://www.youtube.com/embed/lHK6pgHgm-0" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe></center>
</center><br>

<br>
Static hand pose recognition add-on using HOG Adaboost Cascades for my previous Hand Tracking algorithm.
<br><br>

<center><iframe width="640" height="360" src="https://www.youtube.com/embed/Rrh1ruJLD84" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe></center>
</center><br>

<br>
<br>



<hr>
<center><a id="longHandsHalo" name="longHandsHalo"><h3>Long Hands Gesture Recognition controller for Halo</h3></a></center>

<center><a name="HGRCarDriverSubHeader"><h4>Controller for first person video game called Halo. 2015 </h4></a></center>

<div class="tg-wrap"><table class="tg">

    <th class="tg-s6z2"><a href="mailto:ppopo068@uottawa.ca?Subject=Hand Gesture Recognition">Pavel Popov</a>, <a href="mailto:laganier@eecs.uottawa.ca?Subject=Hand Gesture Recognition">Robert Laganiere</a><br>University of Ottawa<br>Ottawa, ON, Canada</th>
          </tr>
  <tr>
    <td class="tg-s6z2" colspan="2">Questions? <a href="mailto:rverb054@uottawa.ca?Subject=HandGestureRecognition&cc=laganier@eecs.uottawa.ca;ppopo068@uottawa.ca">Drop us a line</a></td>
  </tr>
</table></div>

<hr><br>

I made a controller for a first person shooter video game called Halo using my Long Hands Gesture Recognition system. Using two web cameras the system tracks the user's hands. The left hand controls character movement, while the right hand controls aiming and shooting. This served as an important proof of concept that showed that user interfaces capable of rapid response could be made using Long Hands Gesture Recognition. 


<br><br>

<left><a name="Results"><h4>Results</h4></a></left>


Long Hands Gesture Recognition controller for Halo video demonstration
<br><br>

<center><iframe width="640" height="360" src="https://www.youtube.com/embed/NuVAW6wihZ8" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe></center>
</center><br><br>

<hr>
<center><a id="longHands" name="longHands"><h3>Long Hands Gesture Recognition System</h3></a></center>

<center><a name="HGRCarDriverSubHeader"><h4>Contour Shape Analysis and Pixel Colour Based Template Matching for Hybrid Real-Time Gesture Recognition and Tracking. 2015

</h4></a></center>

<div class="tg-wrap"><table class="tg">

    <th class="tg-s6z2"><a href="mailto:ppopo068@uottawa.ca?Subject=Hand Gesture Recognition">Pavel Popov</a>, <a href="mailto:laganier@eecs.uottawa.ca?Subject=Hand Gesture Recognition">Robert Laganiere</a><br>University of Ottawa<br>Ottawa, ON, Canada</th>
          </tr>
  <tr>
    <td class="tg-s6z2" colspan="2">Questions? <a href="mailto:rverb054@uottawa.ca?Subject=HandGestureRecognition&cc=laganier@eecs.uottawa.ca;ppopo068@uottawa.ca">Drop us a line</a></td>
  </tr>
</table></div>

<hr><br>

I integrated my previously developed hand recognition algorithm and my contour based and template based hand tracking algorithms into one system. Unimodal histogram filtering and contour shape analysis works to rapidly detect a hand 5 gesture displayed in the middle of a video frame, and then the two part hand tracking algorithm tracks the hand in subsequent frames. The original contour based tracking algorithm was complemented by adding the template based tracking algorithm. The contour based tracking algorithm partially guides the template based tracking algorithm and this improves the robustness of the template tracking. The template tracking algorithm provides stable tracking of the fingertips improving the system's usability as a user interface input mechanism. The description of the project was drafted as a journal article and a video demonstration was made. My thesis supervisor and I decided to add machine learning to improve the hand gesture recognition and make it position independent before publishing in order to present a more complete system.



<br><br>

<left><a name="Results"><h4>Results</h4></a></left>


Long Hands Gesture Recognition System video demonstration
<br><br>

<center><iframe width="640" height="360" src="https://www.youtube.com/embed/OcgXYQFNkIU" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe></center>
</center><br>

<hr>
<center><a id="GRPaint" name="GRPaint"><h3>Gesture Recognition Paint</h3></a></center>

<center><a name="HGRCarDriverSubHeader"><h4>Android application allowing a user to draw pictures with hand gestures in front of an Android device camera. 2015


</h4></a></center>

<div class="tg-wrap"><table class="tg">

    <th class="tg-s6z2"><a href="mailto:ppopo068@uottawa.ca?Subject=Hand Gesture Recognition">Pavel Popov</a>, <a href="mailto:laganier@eecs.uottawa.ca?Subject=Hand Gesture Recognition">Robert Laganiere</a><br>University of Ottawa<br>Ottawa, ON, Canada</th>
          </tr>
  <tr>
    <td class="tg-s6z2" colspan="2">Questions? <a href="mailto:rverb054@uottawa.ca?Subject=HandGestureRecognition&cc=laganier@eecs.uottawa.ca;ppopo068@uottawa.ca">Drop us a line</a></td>
  </tr>
</table></div>

<hr><br>

I successfully ported my gesture recognition work to an Android Device. Using my hand recognition and contour based hand tracking algorithms I created an App that lets a user draw on the screen of a tablet by making hand gestures in front of the front facing device camera. The App is called Gesture Recognition Paint. It has 6 different colours to choose from when drawing. This important milestone for my thesis research proved that my gesture recognition methods are scalable to different platforms.


<br><br>

<left><a name="Results"><h4>Results</h4></a></left>


Gesture Recognition Paint App created for Android Tablet Devices
<br><br>

<center><iframe width="640" height="360" src="https://www.youtube.com/embed/8e2xZpFhcIY" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe></center>
</center><br>



<center> </center>

		</div>

			</section>
		</div>
		<div class="3u 12u(mobile)">

			<!-- Sidebar -->
				<section>
					<header>
						<h2>Navigation</h2>
					</header>
					<ul class="link-list">
						<li><a href="#dataset_handtrack">Hand Tracking Dataset</a></li>
						<li><a href="#dataset_gesture">Gesture Recognition Dataset</a></li>
						<li><a href="#overview">Overview</a></li>
						<li><a href="#examples" onclick="collapsejump(event)">Projects</a></li>
					     <li><a href="#examples2" onclick="collapsejump(event)">Hand Gesture Recognition for Car Driver Interactions</a></li>
						<li><a href="#longHandsHalo" onclick="collapsejump(event);">Long Hands Halo GR Controller</a></li>
						<li><a href="#longHands" onclick="collapsejump(event)">Long Hands Gesture Recognition</a></li>
<li><a href="#GRPaint" onclick="collapsejump(event)">Gesture Recognition Paint</a></li>

					</ul>
				</section>
				<section>
					<header>
						<h2>Funding</h2>
					</header>
					<p>Thank you to Mitacs and NSERC for making this work possible!</p>
					<center><a href="https://www.mitacs.ca/en" target="_blank"><div class="net_params"><img src="images/Mitacs_colour.jpg" alt="Net param summary" style="width:130 px;height:48px;"></div></a></center>

<center><a href="http://www.nserc-crsng.gc.ca/index_eng.asp" target="_blank"><div class="net_params"><img src="images/nserc_crsng_high.jpg" alt="Net param summary" style="width:123px;height:55px;"></div></a></center>

				</section>
		</div>
		</div></div></div></div>

		</div>

		

			<!-- Footer -->
			<div class="template-footer-container"></div>

		<!-- Scripts -->

	</body>
</html>